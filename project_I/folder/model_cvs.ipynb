{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data and import modules\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn\n",
    "import imblearn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### functions:\n",
    "### feature importance selection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### feature_importance selection \n",
    "def best_importances(xx_train,yy_train,iterations=3,threshold=0.85):\n",
    "    fo = 0\n",
    "    for s in range(iterations):\n",
    "        folds = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "        for train_x,train_y in folds.split(xx_train,yy_train):\n",
    "            train_fold = xx_train.iloc[train_x].copy()\n",
    "            train_fold_y = yy_train.iloc[train_x].copy()\n",
    "        \n",
    "            rus = RandomUnderSampler(0.5,random_state=1)\n",
    "            train_fold, train_fold_y = rus.fit_resample(train_fold,train_fold_y)        \n",
    "            colX = train_fold.columns\n",
    "            colX=colX.drop(  [\"Age\",\"seniority_time\",\"gross_income\",\"first_contract_date\"])\n",
    "        \n",
    "            od = get_dict(train_fold,train_fold_y,colX)\n",
    "            xx_transform = apply_dict(train_fold,od,colX)\n",
    "        \n",
    "            clf = RandomForestClassifier(n_jobs=-1,n_estimators=50,class_weight=\"balanced\")\n",
    "            model_out = clf.fit(xx_transform,train_fold_y)\n",
    "            fI = pd.Series(index=xx_transform.columns,data=model_out.feature_importances_)\n",
    "        \n",
    "            if not isinstance(fo,pd.DataFrame):\n",
    "                fo = pd.DataFrame(fI)\n",
    "            else:\n",
    "                fo = pd.concat([fo,fI],axis=1)\n",
    "    me = fo.mean(axis=1)\n",
    "    me.sort_values(ascending=False,inplace=True)       \n",
    "    selects = me.index[me.cumsum() < threshold]\n",
    "    return(selects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### target encoder \n",
    "\n",
    "class Target_Encoder_I(BaseEstimator,TransformerMixin):\n",
    "    \"\"\" assumes here that for estimation and prediction \n",
    "        values in X have same order; X. is changed to \n",
    "        numpy array; in case no label exists in X \n",
    "        respective na_values are being deleted\"\"\"\n",
    "        \n",
    "    def __init__(self,cols):\n",
    "            self.cols=cols\n",
    "          \n",
    "            \n",
    "    def fit(self,X,y):\n",
    "        Xi = X.copy()\n",
    "        Xi[\"Tar\"] = y.to_numpy()\n",
    "        out_dict = defaultdict(dict)\n",
    "        for ii in self.cols:\n",
    "            out_dict[ii] = Xi.groupby(ii)[\"Tar\"].mean()\n",
    "       \n",
    "        self.out_dict = out_dict\n",
    "        Xi = Xi.drop(\"Tar\",axis=1,inplace=True)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function for custom kfolding that accounts for imbalanced datasets \n",
    "def custom_cv_3(X,y,n_splits=10,test_size=0.05,alternation1=5):\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits,random_state=110,test_size=test_size)\n",
    "    for train_index, test_index in sss.split(X,y):\n",
    "        mask1 = y[train_index] ==1\n",
    "        targets_ix = train_index[mask1]\n",
    "        remainIX = np.delete(train_index,np.where(mask1==1))\n",
    "\n",
    "        comIX =np.random.choice(remainIX,size=int(mask1.sum()*alternation1 ),replace=False)\n",
    "        new_train = np.concatenate((targets_ix,comIX))\n",
    "        yield new_train, test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start with spltting to test/train set\n",
    "\n",
    "##load data\n",
    "df_k = pd.read_pickle(\"cleaned_data.pkl\")\n",
    "## separate targets and features\n",
    "col = df_k.columns[:39].to_list() +[\"new_ones\",\"leave_ones\"]\n",
    "X = df_k.loc[:,col].copy()\n",
    "Y = df_k.loc[:,\"Saving_Account_target\":\"Direct_Debit_target\"].copy()\n",
    "### split to train and validtaion set\n",
    "x_train,x_validation, Y_train,Y_validation = train_test_split(X,Y,test_size=0.3,random_state=123)\n",
    "### select those that have more than 300 positive cases \n",
    "boll = (Y_validation.sum(axis=0) > 300)\n",
    "\n",
    "col_to_val = Y_validation.columns[boll]\n",
    "\n",
    "### define output frames\n",
    "output_frame = pd.DataFrame(columns={\"randomforestclassifier__n_estimators\",\"randomforestclassifier\n",
    "                                     __max_depth\",\"scores\"} )\n",
    "estim = list()\n",
    "colnameX = list()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in col_to_val[]:\n",
    "    ### first recrods that already have the target in the basekt are removed\n",
    "    to_mask = x_train[v.split(\"_target\")[:2][0]]\n",
    "\n",
    "    eliminate_records = x_train.index[np.where(to_mask==1)[0]]\n",
    "\n",
    "    xx_train = x_train.copy()\n",
    "\n",
    "    xx_train.drop(eliminate_records,inplace=True)\n",
    "    \n",
    "    yy_train = Y_train[v].copy()\n",
    "\n",
    "    yy_train.drop(eliminate_records,inplace=True)\n",
    "\n",
    "    xx_train.drop(v.split(\"_target\")[:2][0],axis=1,inplace=True)\n",
    "     \n",
    "    colZ = xx_train.columns[xx_train.dtypes!=\"float\"]  \n",
    "    ### feature importance is calcualted \n",
    "    cols_to = best_importances(xx_train,yy_train,iterations=4,threshold=0.85)\n",
    "    \n",
    "    pickle.dump(cols_to,open(v+\"Feat.pkl\",\"wb\"))\n",
    "    \n",
    "    colZ = xx_train[cols_to].columns[xx_train[cols_to].dtypes!=\"float\"]\n",
    "    ### pipeline is developed \n",
    "    clf = RandomForestClassifier(n_jobs=-1,class_weight=\"balanced\")       \n",
    "    \n",
    "    pipeII = make_pipeline(Target_Encoder_I(cols =colZ),SimpleImputer(missing_values=np.nan,strategy=\"mean\"),clf)\n",
    "    \n",
    "    parameters ={'randomforestclassifier__n_estimators':[50,100,120],'randomforestclassifier__max_depth':[5,10]}\n",
    "             \n",
    "    scoring2 = {\"auc\":make_scorer(roc_auc_score,needs_proba=True),\"f\":make_scorer(f1_score,average=\"weighted\")}\n",
    "    \n",
    "    ks = custom_cv_3(xx_train[cols_to],yy_train,n_splits=10,test_size=0.3,alternation1=1)\n",
    "    \n",
    "    cc= GridSearchCV(pipeII,cv=ks,scoring=scoring2,param_grid=parameters,refit=\"f\")\n",
    "    \n",
    "    cc.fit(xx_train[cols_to],yy_train)\n",
    "    \n",
    "    pd.DataFrame(cc.cv_results_).to_pickle(\"cv_res\"+v+\".pkl\")\n",
    "       \n",
    "    \n",
    "    ### save model\n",
    "    modell = cc.best_estimator_\n",
    "    pickle.dump(modell,open(\"best_est_\"+v+\".sav\",\"wb\"))\n",
    "    estim.append(modell)\n",
    "    pickle.dump(estim,open(\"estimators.sav\",\"wb\"))\n",
    "    \n",
    "    ## pickle.load(open(filename,\"rb\"))\n",
    "\n",
    "    ok = cc.best_params_\n",
    "    ok[\"scores\"] = cc.best_score_\n",
    "    colnameX.append(v)\n",
    "    \n",
    "    output_frame = output_frame.append(ok,ignore_index=True)\n",
    "    \n",
    "    output_frame.index=colnameX\n",
    "    \n",
    "    output_frame.to_pickle(\"actual_est.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##read in estimates form CV and important features \n",
    "params = pd.read_pickle(\"actual_est.pkl\")    \n",
    "\n",
    "fileslist = []\n",
    "for ro,dirs,files in os.walk(\".\"):\n",
    "    fileslist.append(files)\n",
    "featureS = []\n",
    "feat_dict = defaultdict(list)\n",
    "for f in fileslist[0]:\n",
    "    if \"Feat\" in f:\n",
    "        featureS.append(f)\n",
    "        #pd.read_pickle(f)\n",
    "        feat_dict[f.split(\"Feat\")[0]].extend(pd.read_pickle(f))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply fit with best features and parameters to training set and test on validation set \n",
    "out_of_dict_new = defaultdict()\n",
    "out_of_dict=defaultdict(list)\n",
    "for vv in col_to_val:\n",
    "    featS = feat_dict[vv]\n",
    "    xTR = x_train[featS]\n",
    "    yTR = Y_train[vv]\n",
    "    \n",
    "    colToV = xTR.columns[xTR.dtypes != \"float\"]\n",
    "    #x_TR_trans_dict = get_dict(xTR,yTR,columS=colToV)\n",
    "    #x_TR_trans = apply_dict(xTR,x_TR_trans_dict,colToV)    \n",
    "    \n",
    "    paramS = params.loc[vv]\n",
    "    est = int(paramS[\"randomforestclassifier__n_estimators\"])\n",
    "    est1 = int(paramS[\"randomforestclassifier__max_depth\"])\n",
    "\n",
    "    clff = RandomForestClassifier(n_jobs=-1,class_weight=\"balanced\",n_estimators=est,max_depth = est1)\n",
    "    ### here pipeline differs  \n",
    "    pipeII = make_pipeline(RandomUnderSampler(0.5,random_state=101),Target_Encoder_I,clff)\n",
    "\n",
    "    model = pipeII.fit(x_TR,yTR)\n",
    "    \n",
    "    xVal = x_validation[featS]\n",
    "    \n",
    "    yVal = Y_validation[[vv]]\n",
    "    \n",
    "    proba_y = pipeII.predict_proba(xVal)\n",
    "    \n",
    "    roV = roc_auc_score(yVal,proba_y[:,1])\n",
    "    out_of_dict[vv].append(roV)\n",
    "    pickle.dump(out_of_dict,open(\"res_file.pkl\",\"wb\"))\n",
    "    \n",
    "    lab = pd.DataFrame(data={\"True_Y\":yVal,\"Proba_Negative\":proba_y[:,0],\"Proba_Positive\":proba_y[:,1]})\n",
    "    lab.to_pickle(vv+\".pkl\")        \n",
    "\n",
    "\n",
    "oo = pd.Series(out_of_dict).to_frame()\n",
    "oo.columns =[\"Area_under_ROC\"]\n",
    "oo.to_csv(\"results.csv\",sep=\";\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data files and make frame with cv results and out of bag results \n",
    "\n",
    "fi = list()\n",
    "\n",
    "for m,n,p in os.walk(\".\"):\n",
    "    fi.extend(p)\n",
    "odo = defaultdict()    \n",
    "for h in fi:\n",
    "    if \"cv_res\" in h:\n",
    "        print(h)\n",
    "        res = pd.read_pickle(h)\n",
    "        odo[h] = res[[\"mean_test_score\",\"std_test_score\"]].mean(axis=0)\n",
    "    \n",
    "pd.DataFrame(odo).to_csv(\"means_dispersion.csv\",header=True,sep=\";\")\n",
    "cvr = pd.read_pickle(\"cv_resCredit_Card_target.pkl\")\n",
    "\n",
    "\n",
    "lims = pd.DataFrame(odo)\n",
    "cl = lims.columns\n",
    "\n",
    "newcl = [ m.split(\"res\")[1] for m in cl.to_list()]\n",
    "newcl = [m.split(\".\")[0] for m in newcl]\n",
    "lims.columns = newcl\n",
    "\n",
    "res = pd.read_csv(\"results.csv\",sep=\";\")\n",
    "type(res)#resres.s\n",
    "\n",
    "\n",
    "res.iloc[:,1] =res.iloc[:,1].apply(lambda x: pd.to_numeric(x.split(\"[\")[1].split(\"]\")[0]))\n",
    "res.columns = [\"Names\",\"AUC_out_of_bag\"]\n",
    "res.set_index(\"Names\",inplace=True)\n",
    "f=pd.concat([lims.T,res],axis=1)\n",
    "f[\"upper_band\"] = f[\"mean_test_score\"] + 2*f[\"std_test_score\"]\n",
    "f[\"lower_band\"] = f[\"mean_test_score\"] -2 *f[\"std_test_score\"]\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "f[[\"upper_band\",\"lower_band\",\"AUC_out_of_bag\"]].plot(figsize=(10,10),fontsize=10,rot=99,title=\"cv auc roc vs out of bag roc\")\n",
    "\n",
    "\n",
    "\n",
    "round(f,2).to_csv(\"results_2.csv\",sep=\";\",header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
