{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import enchant\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "os.chdir(\"D:/fast_ai/NLP/cap/new_data\")\n",
    "df_k= pd.read_pickle(\"cleaned_data_setIII.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get additional measures for problems in text\n",
    "## a) find non alphanumeric characters\n",
    "##b) look at number of stopwords \n",
    "## c) how many capital letters are in text\n",
    "## d) how many non English tokens are there\n",
    "## e) are there special charcaters like @,!,#...\n",
    "\n",
    "\n",
    "def text_anomaly(x):\n",
    "    '''function that gives back if a) all english b) number non alphanumeric c) number stopwords d) number of upper cases e) seldom characters'''    \n",
    "    \n",
    "    wt = word_tokenize(x)                      \n",
    "    non_alphaNUM = len([i for i in  wt if i.isalpha()==False])\n",
    "    non_English = len([i for i in wt if di.check(i ) == False])\n",
    "    in_stopwords = len([i for i in wt if i in stop_words])\n",
    "    upper_letters = len([i for k in wt for i in k if i.isupper()==True ])## generally how many upper we have\n",
    "    number_special_characters = len([i for i in wt if regex.search(i) != None]) ## special characters.... \n",
    "    \n",
    "    return(non_alphaNUM,non_English,in_stopwords,upper_letters,number_special_characters)\n",
    "    \n",
    "def measure_Anomaly(df):\n",
    "    global di\n",
    "    di = enchant.Dict(\"en\")\n",
    "    global stop_words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    global regex \n",
    "    regex = re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "    \n",
    "    anomal = df[\"reviews.text\"].apply(lambda x: text_anomaly(x))    \n",
    "    measures = pd.DataFrame(list(zip(*anomal.tolist()))).T\n",
    "    measures.columns = [\"num_nonAlphaN\",\"num_nonEnglish\",\"num_stopwords\",\"num_upperLetters\",\"num_special_characters\"]\n",
    "    return measures \n",
    "## we look at ouliers in terms of ratios, i.e. num_of special characters should not be greather than 80% of all characters..\n",
    "## trying to omit text f.i. \"what a cool product!!!!!!!!!!!!!!\" because it does not provide information a) and b) is too \n",
    "## straight for classifciation \n",
    "## though we will use this cleaning step later, when splitting the datasets \n",
    "\n",
    "def outlier_detection(df1):\n",
    "    sub_df =df1.loc[:,[\"num_nonAlphaN\",\"num_nonEnglish\",\"num_stopwords\",\"num_upperLetters\",\"num_special_characters\"]].copy()\n",
    "    s1 = sub_df.div(df1[\"text_word_len\"],axis=0)\n",
    "    ix_toolarge = np.where((s1.iloc[:,0]> 0.8) | (s1.iloc[:,1] >0.8))[0]\n",
    "    ix_too_many = np.where(sub_df[\"num_special_characters\"]/df1[\"text_character_len\"] > 0.3)[0]\n",
    "    ix_too_short = np.where(df1[\"text_word_len\"] < 10 )[0]\n",
    "    ix_elims = ix_too_short.tolist() + ix_toolarge.tolist() + ix_too_many.tolist() \n",
    "\n",
    "    return set(ix_elims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "oF = measure_Anomaly(df_k)\n",
    "oF.index= df_k.index\n",
    "df_k = pd.concat([df_k,oF],axis=1,ignore_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_elim = outlier_detection(df_k)\n",
    "\n",
    "\n",
    "clean_reviews = df_k.drop(df_k.index[list(ix_elim)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.300970\n",
       "1    0.270030\n",
       "3    0.197879\n",
       "2    0.125545\n",
       "4    0.105576\n",
       "Name: reviews.rating, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews[\"reviews.rating\"].value_counts(normalize=True)\n",
    "\n",
    "clean_reviews_1 = clean_reviews.drop_duplicates(\"reviews.text\")\n",
    "\n",
    "clean_reviews_1.to_pickle(\"clean_reviews_1.pkl\")\n",
    "\n",
    "np.random.seed(412)\n",
    "\n",
    "ixx = np.arange(0,clean_reviews_1.shape[0])\n",
    "clean_reviews_1.index =ixx\n",
    "\n",
    "ixx_lm = np.random.choice(range(clean_reviews_1.shape[0]),55000,replace=False)\n",
    "ixx_lm_el = clean_reviews_1.index[ixx_lm]\n",
    "lm_data_set = clean_reviews_1.loc[ixx_lm_el]\n",
    "\n",
    "red_set = clean_reviews_1.drop(index=ixx_lm_el)\n",
    "\n",
    "np.random.seed(123)\n",
    "ix_train = np.random.choice(range(red_set.shape[0]),33000,replace=False)\n",
    "data_train = red_set.iloc[ix_train]\n",
    "data_test = red_set.drop(index= red_set.index[ix_train] )\n",
    "data_test.shape\n",
    "\n",
    "data_test[\"reviews.rating\"].value_counts(normalize=True)\n",
    "data_train[\"reviews.rating\"].value_counts(normalize=True)\n",
    "\n",
    "#lm_data_set.to_pickle(\"language_model_data.pkl\")\n",
    "#data_test.to_pickle(\"test.pkl\")\n",
    "#data_train.to_pickle(\"train.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98918, 31)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
